{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from the files and turning it into a Dataset\n",
    "\n",
    "Inspired by https://github.com/dialogue-evaluation/RuNNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ners</th>\n",
       "      <th>sentences</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0 5 CITY, 16 23 PERSON, 34 41 PERSON, 46 62 L...</td>\n",
       "      <td>Бостон взорвали Тамерлан и Джохар Царнаевы из ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[21 28 PROFESSION, 53 67 ORGANIZATION, 100 148...</td>\n",
       "      <td>Умер избитый до комы гитарист и сооснователь г...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0 4 PERSON, 37 42 COUNTRY, 47 76 ORGANIZATION...</td>\n",
       "      <td>Путин подписал распоряжение о выходе России из...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0 11 PERSON, 36 47 PROFESSION, 49 60 PERSON, ...</td>\n",
       "      <td>Бенедикт XVI носил кардиостимулятор\\nПапа Римс...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0 4 PERSON, 17 29 ORGANIZATION, 48 56 PROFESS...</td>\n",
       "      <td>Обама назначит в Верховный суд латиноамериканк...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>[42 46 COUNTRY, 82 87 COUNTRY, 104 123 LOCATIO...</td>\n",
       "      <td>Глава Малайзии: мы не хотим противостоять Кита...</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>[1 4 PRODUCT, 31 33 FACILITY, 35 44 TIME, 48 6...</td>\n",
       "      <td>«Союз» впервые пристыковался к МКС за 6 часов\\...</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>[0 4 PERSON, 8 12 PERSON, 45 52 AGE, 72 80 PRO...</td>\n",
       "      <td>Трамп и Путин сделали совместное заявление к 7...</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>[0 9 NATIONALITY, 58 72 PERSON, 101 115 PERSON...</td>\n",
       "      <td>Российский магнат устроил самую дорогую свадьб...</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>[0 4 PERSON, 16 25 PROFESSION, 27 38 PERSON, 8...</td>\n",
       "      <td>Трамп поздравил астронавта Пегги Уитсон с уста...</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>519 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ners  \\\n",
       "0    [0 5 CITY, 16 23 PERSON, 34 41 PERSON, 46 62 L...   \n",
       "1    [21 28 PROFESSION, 53 67 ORGANIZATION, 100 148...   \n",
       "2    [0 4 PERSON, 37 42 COUNTRY, 47 76 ORGANIZATION...   \n",
       "3    [0 11 PERSON, 36 47 PROFESSION, 49 60 PERSON, ...   \n",
       "4    [0 4 PERSON, 17 29 ORGANIZATION, 48 56 PROFESS...   \n",
       "..                                                 ...   \n",
       "514  [42 46 COUNTRY, 82 87 COUNTRY, 104 123 LOCATIO...   \n",
       "515  [1 4 PRODUCT, 31 33 FACILITY, 35 44 TIME, 48 6...   \n",
       "516  [0 4 PERSON, 8 12 PERSON, 45 52 AGE, 72 80 PRO...   \n",
       "517  [0 9 NATIONALITY, 58 72 PERSON, 101 115 PERSON...   \n",
       "518  [0 4 PERSON, 16 25 PROFESSION, 27 38 PERSON, 8...   \n",
       "\n",
       "                                             sentences   id  \n",
       "0    Бостон взорвали Тамерлан и Джохар Царнаевы из ...    0  \n",
       "1    Умер избитый до комы гитарист и сооснователь г...    1  \n",
       "2    Путин подписал распоряжение о выходе России из...    2  \n",
       "3    Бенедикт XVI носил кардиостимулятор\\nПапа Римс...    3  \n",
       "4    Обама назначит в Верховный суд латиноамериканк...    4  \n",
       "..                                                 ...  ...  \n",
       "514  Глава Малайзии: мы не хотим противостоять Кита...  514  \n",
       "515  «Союз» впервые пристыковался к МКС за 6 часов\\...  515  \n",
       "516  Трамп и Путин сделали совместное заявление к 7...  516  \n",
       "517  Российский магнат устроил самую дорогую свадьб...  517  \n",
       "518  Трамп поздравил астронавта Пегги Уитсон с уста...  518  \n",
       "\n",
       "[519 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def list2str(labels):\n",
    "    result = []\n",
    "    for i in labels:\n",
    "        result.append(' '.join([str(j) for j in i]))\n",
    "    return result\n",
    "\n",
    "df = pd.read_json(\"public_dat/train.jsonl\", lines=True)\n",
    "df[\"ners\"] = df.apply(lambda x: list2str(x['ners']), axis=1)\n",
    "\n",
    "# type(df['ners'])\n",
    "train_dataset = Dataset.from_pandas(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['senences', 'id'],\n",
       "    num_rows: 65\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df = pd.read_json(\"public_dat/dev.jsonl\", lines=True)\n",
    "\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "\n",
    "dev_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from transformers import (\n",
    "    BertForTokenClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging,\n",
    ")\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "# from iobes_flat_dataset import IOBESFlatRuNNEDataset, collate_to_max_length\n",
    "# from score import Evaluator\n",
    "\n",
    "\n",
    "class BaselineRuBERT(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self, in_path, out_path, tag_to_id, total_steps, lr=1e-4, weight_decay=0.02\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = BertForTokenClassification.from_pretrained(\n",
    "            \"DeepPavlov/rubert-base-cased\", num_labels=29 * 4 + 1, return_dict=False\n",
    "        )\n",
    "\n",
    "        self.lr = lr\n",
    "        self.total_steps = total_steps\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.tag_to_id = tag_to_id\n",
    "\n",
    "        tags = [None] * (max(self.tag_to_id.values()) + 1)\n",
    "        for tag, idx in self.tag_to_id.items():\n",
    "            tags[idx] = tag\n",
    "\n",
    "        self.id_to_tag = tags\n",
    "\n",
    "        self.in_path = in_path\n",
    "        self.out_path = out_path\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": self.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters, betas=(0.9, 0.999), lr=self.lr, eps=1e-6\n",
    "        )\n",
    "\n",
    "        t_total = self.total_steps\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.lr,\n",
    "            pct_start=0.3,\n",
    "            total_steps=t_total,\n",
    "            anneal_strategy=\"linear\",\n",
    "        )\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels=None):\n",
    "\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # batch:\n",
    "        # [\n",
    "        #     torch.LongTensor(tokens),\n",
    "        #     torch.LongTensor(type_ids),\n",
    "        #     torch.LongTensor(labels_ids),\n",
    "        #     torch.LongTensor(offsets),\n",
    "        #     torch.LongTensor([data[\"id\"]]),\n",
    "        #     context,\n",
    "        #     filename,\n",
    "        #     txtdata,\n",
    "        #     tid,\n",
    "        #     c_start,\n",
    "        #     c_end\n",
    "        # ]\n",
    "\n",
    "        tokens, token_type_ids, labels = batch[0], batch[1], batch[2]\n",
    "\n",
    "        attention_mask = (tokens != 0).long()\n",
    "        loss = self(tokens, attention_mask, token_type_ids, labels=labels)[0]\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        training_loss = sum([float(loss_dict[\"loss\"]) for loss_dict in outputs])\n",
    "        print(\"Loss on train: {:.6f}\".format(training_loss))\n",
    "\n",
    "        self.log(\"training_loss\", training_loss)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        (\n",
    "            tokens,\n",
    "            token_type_ids,\n",
    "            labels,\n",
    "            offsets,\n",
    "            ids,\n",
    "            contexts,\n",
    "            filenames,\n",
    "            txtdatas,\n",
    "            tids,\n",
    "            c_starts,\n",
    "            c_ends,\n",
    "        ) = batch\n",
    "\n",
    "        attention_mask = (tokens != 0).long()\n",
    "        loss, logits = self(tokens, attention_mask, token_type_ids, labels=labels)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits,\n",
    "            \"labels\": labels,\n",
    "            \"ids\": ids,\n",
    "            \"offsets\": offsets,\n",
    "            \"contexts\": contexts,\n",
    "            \"filenames\": filenames,\n",
    "            \"txtdatas\": txtdatas,\n",
    "            \"tids\": tids,\n",
    "            \"c_starts\": c_starts,\n",
    "            \"c_ends\": c_ends,\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_ids = []\n",
    "        all_offsets = []\n",
    "        all_contexts = []\n",
    "        all_filenames = []\n",
    "        all_txtdatas = []\n",
    "        all_tids = []\n",
    "        all_c_starts = []\n",
    "        all_c_ends = []\n",
    "\n",
    "        sum_loss = 0.0\n",
    "        for output in outputs:\n",
    "\n",
    "            loss = output[\"loss\"]\n",
    "            logits = output[\"logits\"]\n",
    "            labels = output[\"labels\"]\n",
    "            ids = output[\"ids\"]\n",
    "            offsets = output[\"offsets\"]\n",
    "            contexts = output[\"contexts\"]\n",
    "            filenames = output[\"filenames\"]\n",
    "            txtdatas = output[\"txtdatas\"]\n",
    "            tids = output[\"tids\"]\n",
    "            c_starts = output[\"c_starts\"]\n",
    "            c_ends = output[\"c_ends\"]\n",
    "\n",
    "            preds = torch.argmax(logits, dim=2)\n",
    "\n",
    "            all_preds.extend(list(torch.split(preds, 1)))\n",
    "            all_labels.extend(list(torch.split(labels, 1)))\n",
    "            all_ids.extend(list(torch.split(ids, 1)))\n",
    "            all_offsets.extend(list(torch.split(offsets, 1)))\n",
    "            all_contexts.extend(contexts)\n",
    "            all_filenames.extend(filenames)\n",
    "            all_txtdatas.extend(txtdatas)\n",
    "            all_tids.extend(tids)\n",
    "            all_c_starts.extend(c_starts)\n",
    "            all_c_ends.extend(c_ends)\n",
    "\n",
    "            sum_loss += float(loss)\n",
    "\n",
    "        print(\"\\nLoss on dev: {:.6f}\".format(sum_loss))\n",
    "\n",
    "        self.log(\"validation_loss\", sum_loss)\n",
    "\n",
    "        sorted_zip = sorted(\n",
    "            list(\n",
    "                zip(\n",
    "                    all_ids,\n",
    "                    all_preds,\n",
    "                    all_labels,\n",
    "                    all_offsets,\n",
    "                    all_contexts,\n",
    "                    all_filenames,\n",
    "                    all_txtdatas,\n",
    "                    all_tids,\n",
    "                    all_c_starts,\n",
    "                    all_c_ends,\n",
    "                )\n",
    "            ),\n",
    "            key=lambda x: x[0],\n",
    "        )\n",
    "\n",
    "        summary = self.compute_iobes_score(sorted_zip, mode=\"dev\")\n",
    "\n",
    "        self.log(\"mention_f1\", summary[\"Mention F1\"])\n",
    "        self.log(\"mention_precision\", summary[\"Mention precision\"])\n",
    "        self.log(\"mention_recall\", summary[\"Mention recall\"])\n",
    "        self.log(\"macro_f1\", summary[\"Macro F1\"])\n",
    "        self.log(\"macro_fewshot_f1\", summary[\"Macro F1 few-shot\"])\n",
    "\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_PATH = \"./vocab.txt\"\n",
    "NERS_PATH = \"./eval/ref/ners.txt\"\n",
    "IN_PATH = \"./eval\"\n",
    "OUT_PATH = \"./eval\"\n",
    "\n",
    "TRAIN_PATH = \"../data/train\"\n",
    "DEV_PATH = \"../data/dev\"\n",
    "# TEST_PATH = \"./data/test\"\n",
    "\n",
    "TRAIN_IDS_PATH = \"../public_data/train.jsonl\"\n",
    "DEV_IDS_PATH = \"../public_data/dev.jsonl\"\n",
    "# TEST_IDS_PATH = \"./data/test.jsonl\"\n",
    "\n",
    "CKPT_PATH = \"./checkpoints\"\n",
    "CKPT_FILE = \"./checkpoints/epoch=206-step=37466.ckpt\"\n",
    "\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 1\n",
    "NUM_WORKERS = 8\n",
    "MAX_EPOCHS = 1\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 0.02\n",
    "\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "bertwptokenizer = BertWordPieceTokenizer(VOCAB_PATH, lowercase=False)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "dev_dataloader = DataLoader(\n",
    "    dataset=dev_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "model = BaselineRuBERT(\n",
    "    in_path=IN_PATH,\n",
    "    out_path=OUT_PATH,\n",
    "    tag_to_id=train_dataset.tag_to_id,\n",
    "    total_steps=(len(train_dataset) // BATCH_SIZE) * MAX_EPOCHS,\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=CKPT_PATH,\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"macro_f1\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=[checkpoint_callback],\n",
    "    num_sanity_val_steps=-1,\n",
    "    max_epochs=1,\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloader, dev_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
