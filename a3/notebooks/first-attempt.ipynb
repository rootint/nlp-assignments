{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dev:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03911141a681449390f57ebf64dc5081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'eval/ref/dev.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 543\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;66;03m# filenames = set([dataset[i][-5] for i in range(len(dataset))])\u001b[39;00m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;66;03m# print(len(filenames))\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;66;03m#     print(batch)\u001b[39;00m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 543\u001b[0m     \u001b[43mtest_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 514\u001b[0m, in \u001b[0;36mtest_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_dataset\u001b[39m():\n\u001b[0;32m--> 514\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mIOBESFlatRuNNEDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/public_dat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mners_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mners.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformat_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/public_dat/train.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBertWordPieceTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/vocab.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowercase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;66;03m# print(dataset[50])\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset))\n",
      "Cell \u001b[0;32mIn[19], line 328\u001b[0m, in \u001b[0;36mIOBESFlatRuNNEDataset.__init__\u001b[0;34m(self, dataset_name, dataset_path, ners_path, format_path, in_path, tokenizer, max_length)\u001b[0m\n\u001b[1;32m    314\u001b[0m         entities\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m    315\u001b[0m             [\n\u001b[1;32m    316\u001b[0m                 (e[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m], e[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m], e[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtag\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    317\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m context_group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    318\u001b[0m             ]\n\u001b[1;32m    319\u001b[0m         )\n\u001b[1;32m    321\u001b[0m     lines\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    322\u001b[0m         json\u001b[38;5;241m.\u001b[39mdumps(\n\u001b[1;32m    323\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: context_group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtid\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mners\u001b[39m\u001b[38;5;124m\"\u001b[39m: entities}, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    324\u001b[0m         )\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m     )\n\u001b[0;32m--> 328\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43min_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/ref/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    329\u001b[0m     f\u001b[38;5;241m.\u001b[39mwritelines(lines)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Шаг 3: Фильтруем все сущности, оставляя только плоские (самые внутренние)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'eval/ref/dev.jsonl'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from nltk.data import load\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def collate_to_max_length(batch):\n",
    "    \"\"\"\n",
    "    Расширение всех полей элементов до максимального в батче.\n",
    "\n",
    "    Параметры:\n",
    "    ---------------\n",
    "        batch: батч, где каждый из примеров содержит список следующих элементов:\n",
    "        [\n",
    "           tokens : torch.LongTensor of shape (sequence_length) - токены последовательности после токенизации\n",
    "           type_ids : torch.LongTensor of shape (sequence_length)\n",
    "           labels_ids : torch.LongTensor of shape (sequence_length) - id сущностей в последовательности\n",
    "           offsets : torch.LongTensor of shape (sequence_length, 2) - (start, end) диапазоны токенов в исходной последовательности\n",
    "           id : torch.LongTensor of shape (1) - id контекста (предложения)\n",
    "           context : str - предложение\n",
    "           filename : str - имя файла, откуда взято предложение\n",
    "           txtdata : str - весь текст\n",
    "           tid : int - id всего текста по public_data\n",
    "           c_start : int - символьная позиция начала предложения в тексте\n",
    "           c_end : int - символьная позиция конца предложения в тексте\n",
    "        ]\n",
    "    Возвращает:\n",
    "    ---------------\n",
    "        output: список расширенных примеров батча, shape каждого из которых [batch, max_length] (для offsets - [batch, max_length, 2])\n",
    "    \"\"\"\n",
    "    batch_size = len(batch)\n",
    "    max_length = max(x[0].shape[0] for x in batch)\n",
    "    output = []\n",
    "\n",
    "    for field_idx in range(3):\n",
    "        pad_output = torch.full(\n",
    "            [batch_size, max_length], 0, dtype=batch[0][field_idx].dtype\n",
    "        )\n",
    "        for sample_idx in range(batch_size):\n",
    "            data = batch[sample_idx][field_idx]\n",
    "            pad_output[sample_idx][: data.shape[0]] = data\n",
    "        output.append(pad_output)\n",
    "\n",
    "    pad_output = torch.full([batch_size, max_length, 2], 0, dtype=batch[0][3].dtype)\n",
    "    for sample_idx in range(batch_size):\n",
    "        data = batch[sample_idx][3]\n",
    "        pad_output[sample_idx][: data.shape[0]][:] = data\n",
    "    output.append(pad_output)\n",
    "\n",
    "    output.append(torch.stack([x[-7] for x in batch]))\n",
    "    output.append([x[-6] for x in batch])\n",
    "    output.append([x[-5] for x in batch])\n",
    "    output.append([x[-4] for x in batch])\n",
    "    output.append([x[-3] for x in batch])\n",
    "    output.append([x[-2] for x in batch])\n",
    "    output.append([x[-1] for x in batch])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def brat2data(sentence_tokenizer, format_path, dataset_path):\n",
    "    \"\"\"\n",
    "    Преобразование данных brat в список словарей с сущностями.\n",
    "\n",
    "    Параметры:\n",
    "    ---------------\n",
    "    sentence_tokenizer\n",
    "        Токенизатор по предложениям, разбивающий текст на последовательности.\n",
    "    format_path : str\n",
    "        Путь к файлу из public_data, составляющими отображение из текстов в id\n",
    "    dataset_path : str\n",
    "        Путь к директории с данными в формате brat.\n",
    "\n",
    "    Возвращает:\n",
    "    ---------------\n",
    "        context_groups : List[ContextDict]\n",
    "\n",
    "        ContextDict:\n",
    "            \"id\" : int - id контекста (предложения)\n",
    "            \"context\" : str - контекст (предложение),\n",
    "            \"filename\" : str - имя файла\n",
    "            \"txtdata\" : str - весь текст\n",
    "            \"tid\" : int - id всего текста по public_data\n",
    "            \"c_start\" : int - символьная позиция начала предложения в тексте\n",
    "            \"c_end\" : int - символьная позиция конца предложения в тексте\n",
    "            \"entities\" : List[EntityDict] - все сущности предложения\n",
    "        EntityDict:\n",
    "             \"tag\" : str - имя сущности\n",
    "             \"start\" : int - символьная позиция начала сущности в предложении\n",
    "             \"end\" : int - символьная позиция начала сущности в предложении\n",
    "             \"eid\" : int - порядковый номер сущности в предложении\n",
    "             \"span\" : str - сама сущность (подпоследовательность символов)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Считываем отображение текстов в их id по public_data\n",
    "    with open(format_path, \"r\", encoding=\"UTF-8\") as format_file:\n",
    "        txtdata_to_id = {}\n",
    "        for line in format_file:\n",
    "            if line.strip() != \"\":\n",
    "                line_map = json.loads(line)\n",
    "                text = line_map[\"sentences\"]\n",
    "                tid = line_map[\"id\"]\n",
    "                txtdata_to_id[text] = tid\n",
    "\n",
    "    # Начинаем читать файлы из директории с данными\n",
    "    context_groups = []\n",
    "    c_idx = 0\n",
    "\n",
    "    for ad, dirs, files in os.walk(dataset_path):\n",
    "        for f in tqdm(files):\n",
    "\n",
    "            if f[-4:] == \".txt\":\n",
    "\n",
    "                with open(dataset_path + \"/\" + f, \"r\", encoding=\"UTF-8\") as txtfile:\n",
    "                    txtdata = txtfile.read()\n",
    "\n",
    "                try:\n",
    "                    tid = txtdata_to_id[txtdata]\n",
    "                except KeyError:\n",
    "                    continue  # Такие файлы пропускаем\n",
    "\n",
    "                try:\n",
    "                    annfile = open(\n",
    "                        dataset_path + \"/\" + f[:-4] + \".ann\", \"r\", encoding=\"UTF-8\"\n",
    "                    )\n",
    "\n",
    "                    file_entities = []\n",
    "\n",
    "                    # Шаг 1. Считываем все сущности из файла аннотации\n",
    "                    for line in annfile:\n",
    "                        line_tokens = line.split()\n",
    "                        if (\n",
    "                            len(line_tokens) > 3\n",
    "                            and len(line_tokens[0]) > 1\n",
    "                            and line_tokens[0][0] == \"T\"\n",
    "                        ):\n",
    "                            try:\n",
    "                                file_entities.append(\n",
    "                                    {\n",
    "                                        \"tag\": line_tokens[1],\n",
    "                                        \"start\": int(line_tokens[2]),\n",
    "                                        \"end\": int(line_tokens[3])\n",
    "                                        - 1,  # Все конечные позиции на 1 больше, чем действительный конец сущности в тексте\n",
    "                                    }\n",
    "                                )\n",
    "                            except ValueError:\n",
    "                                pass  # Все неподходящие сущности\n",
    "\n",
    "                    annfile.close()\n",
    "\n",
    "                    # Шаг 2. В каждом файле выделить контексты отдельно друг от друга.\n",
    "\n",
    "                    sentence_spans = sentence_tokenizer.span_tokenize(txtdata)\n",
    "\n",
    "                    for span in sentence_spans:\n",
    "\n",
    "                        span_start, span_end = span\n",
    "                        context = txtdata[span_start:span_end]\n",
    "\n",
    "                        sentence_entities = [\n",
    "                            e\n",
    "                            for e in file_entities\n",
    "                            if e[\"start\"] >= span_start and e[\"end\"] <= span_end\n",
    "                        ]\n",
    "\n",
    "                        # Смещаем все позиции сущностей относительно каждого контекста\n",
    "                        for entity in sentence_entities:\n",
    "\n",
    "                            entity[\"start\"] = entity[\"start\"] - span_start\n",
    "                            entity[\"end\"] = entity[\"end\"] - span_start\n",
    "\n",
    "                        eid = 0\n",
    "                        simple_entities = []\n",
    "\n",
    "                        # Сохраняем все сущности\n",
    "                        for entity in sentence_entities:\n",
    "\n",
    "                            tag = entity[\"tag\"]\n",
    "                            start = entity[\"start\"]\n",
    "                            end = entity[\"end\"]\n",
    "\n",
    "                            simple_entities.append(\n",
    "                                {\n",
    "                                    \"tag\": tag,\n",
    "                                    \"start\": start,\n",
    "                                    \"end\": end,\n",
    "                                    \"eid\": eid,\n",
    "                                    \"span\": context[start : end + 1],\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                            eid += 1\n",
    "\n",
    "                        context_groups.append(\n",
    "                            {\n",
    "                                \"id\": c_idx,\n",
    "                                \"context\": context,\n",
    "                                \"filename\": f[:-4],\n",
    "                                \"txtdata\": txtdata,\n",
    "                                \"tid\": tid,\n",
    "                                \"c_start\": span_start,\n",
    "                                \"c_end\": span_end,\n",
    "                                \"entities\": simple_entities,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                        c_idx += 1  # Перейти к следующему предложению\n",
    "\n",
    "                except FileNotFoundError:\n",
    "\n",
    "                    # print(f\"File '{f[:-4]}.ann' not found.\")\n",
    "\n",
    "                    # Файла аннотации не было найдено, создаем \"пустой\" датасет\n",
    "\n",
    "                    sentence_spans = sentence_tokenizer.span_tokenize(txtdata)\n",
    "\n",
    "                    for span in sentence_spans:\n",
    "\n",
    "                        span_start, span_end = span\n",
    "                        context = txtdata[span_start:span_end]\n",
    "\n",
    "                        context_groups.append(\n",
    "                            {\n",
    "                                \"id\": c_idx,\n",
    "                                \"context\": context,\n",
    "                                \"filename\": f[:-4],\n",
    "                                \"txtdata\": txtdata,\n",
    "                                \"tid\": tid,\n",
    "                                \"c_start\": span_start,\n",
    "                                \"c_end\": span_end,\n",
    "                                \"entities\": [],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "    return context_groups\n",
    "\n",
    "\n",
    "class IOBESFlatRuNNEDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Датасет RuNNE с самыми внутренними (плоскими) внутренними сущностями, представляемые в формате IOBES (из данных brat).\n",
    "    На основе torch.utils.Dataset.\n",
    "\n",
    "    Возвращает элементы в формате\n",
    "    item =\n",
    "    [\n",
    "       tokens : torch.LongTensor of shape (sequence_length) - токены последовательности после токенизации\n",
    "       type_ids : torch.LongTensor of shape (sequence_length)\n",
    "       labels_ids : torch.LongTensor of shape (sequence_length) - id сущностей в последовательности\n",
    "       offsets : torch.LongTensor of shape (sequence_length, 2) - (start, end) диапазоны токенов в исходной последовательности\n",
    "       id : torch.LongTensor of shape (1) - id контекста (предложения)\n",
    "       context : str - предложение\n",
    "       filename : str - имя файла, откуда взято предложение\n",
    "       txtdata : str - весь текст\n",
    "       tid : int - id всего текста по public_data\n",
    "       c_start : int - символьная позиция начала предложения в тексте\n",
    "       c_end : int - символьная позиция конца предложения в тексте\n",
    "    ]\n",
    "\n",
    "    Аргументы для инициализации:\n",
    "    ----------------\n",
    "    dataset_name : str\n",
    "        Название датасета (для сохранения в <dataset_name>.jsonl файл)\n",
    "    dataset_path : str\n",
    "        Путь к директории с данными в формате brat.\n",
    "    ners_path : str\n",
    "        Путь к файлу ners.txt с именами всех видов сущностей.\n",
    "    format_path : str\n",
    "        Путь к файлу из public_data, составляющими отображение из текстов в id\n",
    "    in_path : str\n",
    "        Путь к директории по формату Evaluator с ref, res и т.п.\n",
    "    tokenizer\n",
    "        Токенайзер по схеме WordPiece.\n",
    "    max_length : int\n",
    "        Максимальная длина предложений.\n",
    "    ----------------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name,\n",
    "        dataset_path,\n",
    "        ners_path,\n",
    "        format_path,\n",
    "        in_path,\n",
    "        tokenizer,\n",
    "        max_length,\n",
    "    ):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Шаг 1. Считываем все данные в формате brat в список словарей, удобным для обработки.\n",
    "\n",
    "        print(f\"Loading {dataset_name}:\")\n",
    "        self.all_data = brat2data(\n",
    "            load(\"tokenizers/punkt/russian.pickle\"), format_path, dataset_path\n",
    "        )\n",
    "\n",
    "        # Шаг 2. Сохраняем ground-truth данные в <in_path>/ref/<dataset_name>.jsonl\n",
    "\n",
    "        lines = []\n",
    "        all_tids = sorted(list(set([c[\"tid\"] for c in self.all_data])))\n",
    "        for tid in all_tids:\n",
    "\n",
    "            entities = []\n",
    "            for context_group in [data for data in self.all_data if data[\"tid\"] == tid]:\n",
    "                entities.extend(\n",
    "                    [\n",
    "                        (e[\"start\"], e[\"end\"], e[\"tag\"])\n",
    "                        for e in context_group[\"entities\"]\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            lines.append(\n",
    "                json.dumps(\n",
    "                    {\"id\": context_group[\"tid\"], \"ners\": entities}, ensure_ascii=False\n",
    "                )\n",
    "                + \"\\n\"\n",
    "            )\n",
    "\n",
    "        with open(in_path + f\"/ref/{dataset_name}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(lines)\n",
    "\n",
    "        # Шаг 3: Фильтруем все сущности, оставляя только плоские (самые внутренние)\n",
    "\n",
    "        flat_data = []\n",
    "\n",
    "        for c in self.all_data:\n",
    "\n",
    "            flats = []\n",
    "\n",
    "            for ec in c[\"entities\"]:\n",
    "\n",
    "                start = ec[\"start\"]\n",
    "                end = ec[\"end\"]\n",
    "\n",
    "                if (\n",
    "                    len(\n",
    "                        [\n",
    "                            e\n",
    "                            for e in c[\"entities\"]\n",
    "                            if e[\"start\"] >= start\n",
    "                            and e[\"end\"] < end\n",
    "                            or e[\"start\"] > start\n",
    "                            and e[\"end\"] <= end\n",
    "                        ]\n",
    "                    )\n",
    "                    == 0\n",
    "                ):\n",
    "\n",
    "                    flats.append(ec)\n",
    "\n",
    "            flat_data.append({**c, \"entities\": flats})\n",
    "\n",
    "        self.flat_data = flat_data\n",
    "\n",
    "        # Шаг 4. Токенизация по WordPiece и валидация всех сущностей (проверка на ошибки)\n",
    "\n",
    "        new_data = []\n",
    "        for c in self.flat_data:\n",
    "\n",
    "            context = c[\"context\"]\n",
    "\n",
    "            encodings = self.tokenizer.encode(context)\n",
    "            tokens = encodings.ids\n",
    "            type_ids = encodings.type_ids\n",
    "            offsets = encodings.offsets\n",
    "\n",
    "            origin_offset2token_idx_start = {}\n",
    "            origin_offset2token_idx_end = {}\n",
    "            for token_idx in range(len(tokens)):\n",
    "\n",
    "                token_start, token_end = offsets[token_idx]\n",
    "                # skip [CLS] or [SEP]\n",
    "                if token_start == token_end == 0:\n",
    "                    continue\n",
    "\n",
    "                token_end -= 1\n",
    "\n",
    "                origin_offset2token_idx_start[token_start] = token_idx\n",
    "                origin_offset2token_idx_end[token_end] = token_idx\n",
    "\n",
    "            valid_entities = []\n",
    "            for e in c[\"entities\"]:\n",
    "\n",
    "                start = e[\"start\"]\n",
    "                end = e[\"end\"]\n",
    "\n",
    "                try:\n",
    "                    new_start = origin_offset2token_idx_start[start]\n",
    "                    new_end = origin_offset2token_idx_end[end]\n",
    "                except KeyError:\n",
    "                    # Пример некорректен из-за опечатки, обнуляем наличие сущности (пропускаем)\n",
    "                    continue\n",
    "\n",
    "                valid_entities.append({**e, \"tl_start\": new_start, \"tl_end\": new_end})\n",
    "            new_data.append(\n",
    "                {\n",
    "                    **c,\n",
    "                    \"entities\": valid_entities,\n",
    "                    \"tokens\": tokens,\n",
    "                    \"type_ids\": type_ids,\n",
    "                    \"offsets\": offsets,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        self.flat_data = new_data\n",
    "\n",
    "        # Шаг 5: Строим последовательность меток по формату IOBES\n",
    "\n",
    "        labeled_data = []\n",
    "        for c in self.flat_data:\n",
    "\n",
    "            labels = []\n",
    "\n",
    "            for e in c[\"entities\"]:\n",
    "                labels.append((e[\"tag\"], e[\"tl_start\"], e[\"tl_end\"]))\n",
    "\n",
    "            label_seq = [\"O\"] * len(c[\"tokens\"])\n",
    "\n",
    "            ### iobes\n",
    "\n",
    "            for tag, start, end in labels:\n",
    "\n",
    "                if start == end:  # S\n",
    "                    label_seq[start] = \"S-\" + tag\n",
    "                else:  # BI*E\n",
    "                    label_seq[start] = \"B-\" + tag\n",
    "                    label_seq[end] = \"E-\" + tag\n",
    "                    for i in range(start + 1, end):\n",
    "                        label_seq[i] = \"I-\" + tag\n",
    "\n",
    "            labeled_data.append({**c, \"labels\": label_seq})\n",
    "\n",
    "        self.data = labeled_data\n",
    "\n",
    "        # Шаг 6. Загружаем все типы сущностей и присваиваем им id\n",
    "\n",
    "        with open(ners_path, \"r\", encoding=\"UTF-8\") as ners_file:\n",
    "\n",
    "            tags = [t.strip() for t in ners_file.read().split(\"\\n\")]\n",
    "\n",
    "            tag_to_id = {}\n",
    "            for idx, tag in enumerate(tags):\n",
    "                tag_to_id[\"B-\" + tag] = idx * 4 + 1\n",
    "                tag_to_id[\"I-\" + tag] = idx * 4 + 2\n",
    "                tag_to_id[\"E-\" + tag] = idx * 4 + 3\n",
    "                tag_to_id[\"S-\" + tag] = idx * 4 + 4\n",
    "            tag_to_id[\"O\"] = 0\n",
    "\n",
    "            self.tag_to_id = tag_to_id\n",
    "\n",
    "        ###\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        data = self.data[item]\n",
    "\n",
    "        assert len(data[\"tokens\"]) == len(data[\"type_ids\"])\n",
    "        assert len(data[\"type_ids\"]) == len(data[\"labels\"])\n",
    "\n",
    "        labels_ids = [self.tag_to_id[tag] for tag in data[\"labels\"]]\n",
    "\n",
    "        tokens = data[\"tokens\"]\n",
    "        type_ids = data[\"type_ids\"]\n",
    "        offsets = data[\"offsets\"]\n",
    "\n",
    "        context = data[\"context\"]\n",
    "        filename = data[\"filename\"]\n",
    "        txtdata = data[\"txtdata\"]\n",
    "        tid = data[\"tid\"]\n",
    "        c_start = data[\"c_start\"]\n",
    "        c_end = data[\"c_end\"]\n",
    "\n",
    "        # truncate\n",
    "        tokens = tokens[: self.max_length]\n",
    "        type_ids = type_ids[: self.max_length]\n",
    "        labels_ids = labels_ids[: self.max_length]\n",
    "        offsets = offsets[: self.max_length]\n",
    "\n",
    "        sep_token = self.tokenizer.token_to_id(\"[SEP]\")\n",
    "        if tokens[-1] != sep_token:\n",
    "            assert len(tokens) == self.max_length\n",
    "            tokens = tokens[:-1] + [sep_token]\n",
    "\n",
    "        return [\n",
    "            torch.LongTensor(tokens),\n",
    "            torch.LongTensor(type_ids),\n",
    "            torch.LongTensor(labels_ids),\n",
    "            torch.LongTensor(offsets),\n",
    "            torch.LongTensor([data[\"id\"]]),\n",
    "            context,\n",
    "            filename,\n",
    "            txtdata,\n",
    "            tid,\n",
    "            c_start,\n",
    "            c_end,\n",
    "        ]\n",
    "\n",
    "\n",
    "def test_dataset():\n",
    "\n",
    "    dataset = IOBESFlatRuNNEDataset(\n",
    "        dataset_name=\"dev\",\n",
    "        dataset_path=\"../data/public_dat\",\n",
    "        ners_path=\"ners.txt\",\n",
    "        format_path=\"../data/public_dat/train.jsonl\",\n",
    "        in_path=\"eval\",\n",
    "        tokenizer=BertWordPieceTokenizer(\"../data/vocab.txt\", lowercase=False),\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    # print(dataset[50])\n",
    "    print(len(dataset))\n",
    "    print(dataset[10])\n",
    "    # filenames = set([dataset[i][-5] for i in range(len(dataset))])\n",
    "    # print(len(filenames))\n",
    "\n",
    "    # dataloader = DataLoader(\n",
    "    #     dataset=dataset,\n",
    "    #     batch_size=2,\n",
    "    #     shuffle=False,\n",
    "    #     collate_fn=collate_to_max_length\n",
    "    # )\n",
    "\n",
    "    # for batch in dataloader:\n",
    "    #     print(batch)\n",
    "    #     break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to your JSONL file\n",
    "file_path = '../data/public_dat/train.jsonl'\n",
    "\n",
    "train_data = []\n",
    "\n",
    "# Open the file in reading mode\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    # Read lines\n",
    "    for line in file:\n",
    "        # Parse JSON from each line\n",
    "        data = json.loads(line)\n",
    "        # Print the data to check or process it\n",
    "        train_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Бостон взорвали Тамерлан и Джохар Царнаевы из Северного Кавказа\n",
      "\n",
      "19 апреля 2013 года в пригороде Бостона  проходит спецоперация по поимке 19-летнего Джохара Царнаева, подозреваемого в теракте на Бостонском марафоне 15 апреля и в смертельном ранении полицейского на кампусе Массачусетского технологического института 18 апреля.\n",
      "\n",
      "Второй подозреваемый, его брат, 26-летний Тамерлан Царнаев, был ранен в перестрелке в Уотертауне  и позже скончался в больнице.\n",
      "\n",
      "Уотертаун и его окрестности фактически переведены на осадное положение: окрестности оцеплены, дороги перекрыты, магазины и бизнесы закрыты, жителей просят не выходить из домов и не приближаться к окнам, над районом спецоперации перекрыты полёты авиации.\n",
      "\n",
      "В Бостоне приостановлена работа общественного транспорта, включая метро, автобусы, такси и пригородные поезда. Отменены занятия в Гарварде, Массачусетском технологическом институте, Университете Саффолка, Бостонском университете и во всех городских школах.\n",
      "\n",
      "На сайте ФБР опубликованы фото и видео разыскиваемого.\n",
      "\n",
      "19-летний студент Университета штата Массачусетс Джохар Царнаев обучался в средней школе Махачкалы, затем в школе Кембриджа (район Бостона), входил в список стипендиатов Кембриджа.\n",
      "\n",
      "Секретарь директора школы № 1 Махачкалы Ирина Бандурина подтвердила «Эху Москвы», что Джохар Царнаев учился в данном учебном заведении, но покинул его, не закончив первый класс.\n",
      "\n",
      "Один из братьев приехал в США вместе с родителями в 2002 году, а другой — самостоятельно в 2004 году.\n",
      "\n",
      "Инцидент произошел спустя несколько дней после теракта на Бостонском марафоне, во время которого прогремели два взрыва.\n",
      "Их жертвами стали три человека, более 180 получили ранения.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 5, 'CITY'],\n",
       " [16, 23, 'PERSON'],\n",
       " [34, 41, 'PERSON'],\n",
       " [46, 62, 'LOCATION'],\n",
       " [115, 136, 'EVENT'],\n",
       " [138, 147, 'AGE'],\n",
       " [149, 164, 'PERSON'],\n",
       " [195, 213, 'EVENT'],\n",
       " [215, 223, 'DATE'],\n",
       " [273, 314, 'ORGANIZATION'],\n",
       " [316, 324, 'DATE'],\n",
       " [328, 333, 'ORDINAL'],\n",
       " [360, 368, 'AGE'],\n",
       " [370, 385, 'PERSON'],\n",
       " [400, 410, 'EVENT'],\n",
       " [414, 423, 'CITY'],\n",
       " [457, 465, 'CITY'],\n",
       " [714, 720, 'CITY'],\n",
       " [842, 849, 'ORGANIZATION'],\n",
       " [852, 891, 'ORGANIZATION'],\n",
       " [894, 914, 'ORGANIZATION'],\n",
       " [917, 939, 'ORGANIZATION'],\n",
       " [979, 981, 'ORGANIZATION'],\n",
       " [1026, 1034, 'AGE'],\n",
       " [1044, 1073, 'ORGANIZATION'],\n",
       " [1075, 1088, 'PERSON'],\n",
       " [1115, 1123, 'CITY'],\n",
       " [1157, 1163, 'CITY'],\n",
       " [1208, 1226, 'PROFESSION'],\n",
       " [1248, 1262, 'PERSON'],\n",
       " [1277, 1286, 'ORGANIZATION'],\n",
       " [1294, 1307, 'PERSON'],\n",
       " [1372, 1377, 'ORDINAL'],\n",
       " [1413, 1415, 'COUNTRY'],\n",
       " [1437, 1447, 'DATE'],\n",
       " [1476, 1486, 'DATE'],\n",
       " [1537, 1543, 'EVENT'],\n",
       " [1548, 1566, 'EVENT'],\n",
       " [1598, 1600, 'NUMBER'],\n",
       " [1628, 1630, 'NUMBER'],\n",
       " [1642, 1650, 'NUMBER'],\n",
       " [7, 14, 'EVENT'],\n",
       " [65, 78, 'DATE'],\n",
       " [97, 103, 'CITY'],\n",
       " [184, 190, 'CRIME'],\n",
       " [229, 260, 'CRIME'],\n",
       " [273, 287, 'STATE_OR_PROVINCE'],\n",
       " [392, 396, 'EVENT'],\n",
       " [434, 442, 'EVENT'],\n",
       " [1063, 1073, 'STATE_OR_PROVINCE'],\n",
       " [1140, 1148, 'DISTRICT'],\n",
       " [1196, 1204, 'DISTRICT'],\n",
       " [1238, 1246, 'CITY'],\n",
       " [27, 41, 'PERSON'],\n",
       " [16, 41, 'FAMILY'],\n",
       " [87, 103, 'LOCATION'],\n",
       " [852, 865, 'STATE_OR_PROVINCE'],\n",
       " [907, 914, 'CITY'],\n",
       " [917, 926, 'CITY'],\n",
       " [1183, 1194, 'AWARD'],\n",
       " [1183, 1204, 'AWARD'],\n",
       " [1208, 1246, 'PROFESSION'],\n",
       " [1228, 1246, 'ORGANIZATION'],\n",
       " [1661, 1667, 'EVENT'],\n",
       " [1613, 1626, 'EVENT'],\n",
       " [1602, 1607, 'EVENT'],\n",
       " [1548, 1557, 'CITY'],\n",
       " [1559, 1566, 'EVENT'],\n",
       " [56, 62, 'LOCATION'],\n",
       " [195, 204, 'CITY']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data[0]['sentences'])\n",
    "train_data[0]['ners']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
