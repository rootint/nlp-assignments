{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from nltk.data import load\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "def collate_to_max_length(batch):\n",
    "    \"\"\"\n",
    "    Расширение всех полей элементов до максимального в батче.\n",
    "\n",
    "    Параметры:\n",
    "    ---------------\n",
    "        batch: батч, где каждый из примеров содержит список следующих элементов:\n",
    "        [\n",
    "           tokens : torch.LongTensor of shape (sequence_length) - токены последовательности после токенизации\n",
    "           type_ids : torch.LongTensor of shape (sequence_length)\n",
    "           labels_ids : torch.LongTensor of shape (sequence_length) - id сущностей в последовательности\n",
    "           offsets : torch.LongTensor of shape (sequence_length, 2) - (start, end) диапазоны токенов в исходной последовательности\n",
    "           id : torch.LongTensor of shape (1) - id контекста (предложения)\n",
    "           context : str - предложение\n",
    "           filename : str - имя файла, откуда взято предложение\n",
    "           txtdata : str - весь текст\n",
    "           tid : int - id всего текста по public_data\n",
    "           c_start : int - символьная позиция начала предложения в тексте\n",
    "           c_end : int - символьная позиция конца предложения в тексте\n",
    "        ]\n",
    "    Возвращает:\n",
    "    ---------------\n",
    "        output: список расширенных примеров батча, shape каждого из которых [batch, max_length] (для offsets - [batch, max_length, 2])\n",
    "    \"\"\"\n",
    "    batch_size = len(batch)\n",
    "    max_length = max(x[0].shape[0] for x in batch)\n",
    "    output = []\n",
    "\n",
    "    for field_idx in range(3):\n",
    "        pad_output = torch.full([batch_size, max_length], 0, dtype=batch[0][field_idx].dtype)\n",
    "        for sample_idx in range(batch_size):\n",
    "            data = batch[sample_idx][field_idx]\n",
    "            pad_output[sample_idx][: data.shape[0]] = data\n",
    "        output.append(pad_output)\n",
    "\n",
    "    pad_output = torch.full([batch_size, max_length, 2], 0, dtype=batch[0][3].dtype)\n",
    "    for sample_idx in range(batch_size):\n",
    "        data = batch[sample_idx][3]\n",
    "        pad_output[sample_idx][: data.shape[0]][:] = data\n",
    "    output.append(pad_output)\n",
    "\n",
    "    output.append(torch.stack([x[-7] for x in batch]))\n",
    "    output.append([x[-6] for x in batch])\n",
    "    output.append([x[-5] for x in batch])\n",
    "    output.append([x[-4] for x in batch])\n",
    "    output.append([x[-3] for x in batch])\n",
    "    output.append([x[-2] for x in batch])\n",
    "    output.append([x[-1] for x in batch])\n",
    "\n",
    "    return output\n",
    "\n",
    "def brat2data(sentence_tokenizer, format_path, dataset_path):\n",
    "    \"\"\"\n",
    "    Преобразование данных brat в список словарей с сущностями. \n",
    "\n",
    "    Параметры:\n",
    "    ---------------\n",
    "    sentence_tokenizer\n",
    "        Токенизатор по предложениям, разбивающий текст на последовательности. \n",
    "    format_path : str\n",
    "        Путь к файлу из public_data, составляющими отображение из текстов в id\n",
    "    dataset_path : str\n",
    "        Путь к директории с данными в формате brat. \n",
    "\n",
    "    Возвращает:\n",
    "    ---------------\n",
    "        context_groups : List[ContextDict]\n",
    "\n",
    "        ContextDict: \n",
    "            \"id\" : int - id контекста (предложения) \n",
    "            \"context\" : str - контекст (предложение),\n",
    "            \"filename\" : str - имя файла\n",
    "            \"txtdata\" : str - весь текст\n",
    "            \"tid\" : int - id всего текста по public_data\n",
    "            \"c_start\" : int - символьная позиция начала предложения в тексте\n",
    "            \"c_end\" : int - символьная позиция конца предложения в тексте\n",
    "            \"entities\" : List[EntityDict] - все сущности предложения\n",
    "        EntityDict:\n",
    "             \"tag\" : str - имя сущности\n",
    "             \"start\" : int - символьная позиция начала сущности в предложении\n",
    "             \"end\" : int - символьная позиция начала сущности в предложении\n",
    "             \"eid\" : int - порядковый номер сущности в предложении\n",
    "             \"span\" : str - сама сущность (подпоследовательность символов)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Считываем отображение текстов в их id по public_data\n",
    "    with open(format_path, \"r\", encoding = 'UTF-8') as format_file:\n",
    "        txtdata_to_id = {}\n",
    "        for line in format_file:\n",
    "            if line.strip() != '':\n",
    "                line_map = json.loads(line)\n",
    "                text = line_map[\"sentences\"]\n",
    "                tid = line_map[\"id\"]\n",
    "                txtdata_to_id[text] = tid\n",
    "\n",
    "\n",
    "    # Начинаем читать файлы из директории с данными\n",
    "    context_groups = []\n",
    "    c_idx = 0\n",
    "\n",
    "    for ad, dirs, files in os.walk(dataset_path):\n",
    "        for f in tqdm(files):\n",
    "\n",
    "            if f[-4:] == '.txt':\n",
    "\n",
    "                with open(dataset_path + '/' + f, \"r\", encoding='UTF-8') as txtfile:\n",
    "                    txtdata = txtfile.read()\n",
    "\n",
    "                try:\n",
    "                    tid = txtdata_to_id[txtdata]\n",
    "                except KeyError:\n",
    "                    continue # Такие файлы пропускаем\n",
    "\n",
    "                try:\n",
    "                    annfile = open(dataset_path + '/' + f[:-4] + \".ann\", \"r\", encoding='UTF-8')\n",
    "\n",
    "                    file_entities = []\n",
    "\n",
    "                    # Шаг 1. Считываем все сущности из файла аннотации\n",
    "                    for line in annfile:\n",
    "                        line_tokens = line.split()\n",
    "                        if len(line_tokens) > 3 and len(line_tokens[0]) > 1 and line_tokens[0][0] == 'T':\n",
    "                            try:\n",
    "                                file_entities.append({ \n",
    "                                    \"tag\" : line_tokens[1], \n",
    "                                    \"start\" : int(line_tokens[2]),\n",
    "                                    \"end\" : int(line_tokens[3]) - 1, # Все конечные позиции на 1 больше, чем действительный конец сущности в тексте\n",
    "                                })\n",
    "                            except ValueError:\n",
    "                                pass # Все неподходящие сущности\n",
    "\n",
    "                    annfile.close()\n",
    "\n",
    "                    # Шаг 2. В каждом файле выделить контексты отдельно друг от друга.\n",
    "\n",
    "                    sentence_spans = sentence_tokenizer.span_tokenize(txtdata)\n",
    "\n",
    "                    for span in sentence_spans:\n",
    "\n",
    "                        span_start, span_end = span\n",
    "                        context = txtdata[span_start : span_end]\n",
    "\n",
    "                        sentence_entities = [e for e in file_entities if e[\"start\"] >= span_start and e[\"end\"] <= span_end]\n",
    "\n",
    "                        # Смещаем все позиции сущностей относительно каждого контекста\n",
    "                        for entity in sentence_entities:\n",
    "\n",
    "                            entity[\"start\"] = entity[\"start\"] - span_start\n",
    "                            entity[\"end\"] = entity[\"end\"] - span_start\n",
    "\n",
    "                        eid = 0\n",
    "                        simple_entities = []\n",
    "\n",
    "                        # Сохраняем все сущности\n",
    "                        for entity in sentence_entities:\n",
    "\n",
    "                            tag = entity[\"tag\"]\n",
    "                            start = entity[\"start\"]\n",
    "                            end = entity[\"end\"]\n",
    "\n",
    "                            simple_entities.append({\n",
    "                                \"tag\" : tag,\n",
    "                                \"start\" : start,\n",
    "                                \"end\" : end,\n",
    "                                \"eid\" : eid,\n",
    "                                \"span\" : context[start : end + 1]\n",
    "                            })\n",
    "\n",
    "                            eid += 1\n",
    "\n",
    "                        context_groups.append({\n",
    "                            \"id\" : c_idx,\n",
    "                            \"context\" : context,\n",
    "                            \"filename\" : f[:-4],\n",
    "                            \"txtdata\" : txtdata,\n",
    "                            \"tid\" : tid,\n",
    "                            \"c_start\" : span_start,\n",
    "                            \"c_end\": span_end,\n",
    "                            \"entities\" : simple_entities\n",
    "                        })  \n",
    "\n",
    "                        c_idx += 1 # Перейти к следующему предложению              \n",
    "\n",
    "                except FileNotFoundError:\n",
    "\n",
    "                    # print(f\"File '{f[:-4]}.ann' not found.\")\n",
    "\n",
    "                    # Файла аннотации не было найдено, создаем \"пустой\" датасет\n",
    "                    \n",
    "                    sentence_spans = sentence_tokenizer.span_tokenize(txtdata)\n",
    "\n",
    "                    for span in sentence_spans:\n",
    "\n",
    "                        span_start, span_end = span\n",
    "                        context = txtdata[span_start : span_end]\n",
    "\n",
    "                        context_groups.append({\n",
    "                            \"id\" : c_idx,\n",
    "                            \"context\" : context,\n",
    "                            \"filename\" : f[:-4],\n",
    "                            \"txtdata\" : txtdata,\n",
    "                            \"tid\" : tid,\n",
    "                            \"c_start\" : span_start,\n",
    "                            \"c_end\": span_end,\n",
    "                            \"entities\" : []\n",
    "                        })\n",
    "\n",
    "    return context_groups\n",
    "\n",
    "class IOBESFlatRuNNEDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Датасет RuNNE с самыми внутренними (плоскими) внутренними сущностями, представляемые в формате IOBES (из данных brat).\n",
    "    На основе torch.utils.Dataset.\n",
    "\n",
    "    Возвращает элементы в формате   \n",
    "    item = \n",
    "    [\n",
    "       tokens : torch.LongTensor of shape (sequence_length) - токены последовательности после токенизации\n",
    "       type_ids : torch.LongTensor of shape (sequence_length)\n",
    "       labels_ids : torch.LongTensor of shape (sequence_length) - id сущностей в последовательности\n",
    "       offsets : torch.LongTensor of shape (sequence_length, 2) - (start, end) диапазоны токенов в исходной последовательности\n",
    "       id : torch.LongTensor of shape (1) - id контекста (предложения)\n",
    "       context : str - предложение\n",
    "       filename : str - имя файла, откуда взято предложение\n",
    "       txtdata : str - весь текст\n",
    "       tid : int - id всего текста по public_data\n",
    "       c_start : int - символьная позиция начала предложения в тексте\n",
    "       c_end : int - символьная позиция конца предложения в тексте\n",
    "    ]\n",
    "    \n",
    "    Аргументы для инициализации:\n",
    "    ----------------\n",
    "    dataset_name : str\n",
    "        Название датасета (для сохранения в <dataset_name>.jsonl файл)\n",
    "    dataset_path : str\n",
    "        Путь к директории с данными в формате brat. \n",
    "    ners_path : str\n",
    "        Путь к файлу ners.txt с именами всех видов сущностей.\n",
    "    format_path : str\n",
    "        Путь к файлу из public_data, составляющими отображение из текстов в id\n",
    "    in_path : str\n",
    "        Путь к директории по формату Evaluator с ref, res и т.п.\n",
    "    tokenizer\n",
    "        Токенайзер по схеме WordPiece.\n",
    "    max_length : int\n",
    "        Максимальная длина предложений. \n",
    "    ----------------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_name, dataset_path, ners_path, format_path, in_path, tokenizer, max_length):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Шаг 1. Считываем все данные в формате brat в список словарей, удобным для обработки. \n",
    "\n",
    "        print(f\"Loading {dataset_name}:\")\n",
    "        self.all_data = brat2data(load(\"tokenizers/punkt/russian.pickle\"), format_path, dataset_path)\n",
    "\n",
    "        # Шаг 2. Сохраняем ground-truth данные в <in_path>/ref/<dataset_name>.jsonl \n",
    "\n",
    "        lines = []\n",
    "        all_tids = sorted(list(set([c[\"tid\"] for c in self.all_data])))\n",
    "        for tid in all_tids:\n",
    "\n",
    "            entities = []\n",
    "            for context_group in [data for data in self.all_data if data[\"tid\"] == tid]:\n",
    "                entities.extend([(e[\"start\"], e[\"end\"], e[\"tag\"]) for e in context_group[\"entities\"]])\n",
    "\n",
    "            lines.append(json.dumps({\"id\" : context_group[\"tid\"], \"ners\" : entities}, ensure_ascii = False) + '\\n')\n",
    "\n",
    "        with open(in_path + f'/ref/{dataset_name}.jsonl', 'w', encoding=\"utf-8\") as f:\n",
    "            f.writelines(lines)\n",
    "\n",
    "        # Шаг 3: Фильтруем все сущности, оставляя только плоские (самые внутренние)\n",
    "\n",
    "        flat_data = []\n",
    "\n",
    "        for c in self.all_data:\n",
    "\n",
    "            flats = []\n",
    "\n",
    "            for ec in c[\"entities\"]:\n",
    "\n",
    "                start = ec[\"start\"]\n",
    "                end = ec[\"end\"]\n",
    "\n",
    "                if len([e for e in c[\"entities\"] if e[\"start\"] >= start and e[\"end\"] < end or e[\"start\"] > start and e[\"end\"] <= end]) == 0:\n",
    "\n",
    "                    flats.append(ec)\n",
    "\n",
    "            flat_data.append({**c, \"entities\" : flats})\n",
    "\n",
    "        self.flat_data = flat_data\n",
    "\n",
    "        # Шаг 4. Токенизация по WordPiece и валидация всех сущностей (проверка на ошибки)\n",
    "\n",
    "        new_data = []\n",
    "        for c in self.flat_data:\n",
    "\n",
    "            context = c[\"context\"]\n",
    "\n",
    "            encodings = self.tokenizer.encode(context)\n",
    "            tokens = encodings.ids\n",
    "            type_ids = encodings.type_ids\n",
    "            offsets = encodings.offsets\n",
    "\n",
    "            origin_offset2token_idx_start = {}\n",
    "            origin_offset2token_idx_end = {}\n",
    "            for token_idx in range(len(tokens)):\n",
    "\n",
    "                token_start, token_end = offsets[token_idx]\n",
    "                # skip [CLS] or [SEP]\n",
    "                if token_start == token_end == 0:\n",
    "                    continue\n",
    "\n",
    "                token_end -= 1\n",
    "\n",
    "                origin_offset2token_idx_start[token_start] = token_idx\n",
    "                origin_offset2token_idx_end[token_end] = token_idx\n",
    "            \n",
    "            valid_entities = []\n",
    "            for e in c[\"entities\"]:\n",
    "\n",
    "                start = e[\"start\"]\n",
    "                end = e[\"end\"]\n",
    "\n",
    "                try:\n",
    "                    new_start = origin_offset2token_idx_start[start]\n",
    "                    new_end = origin_offset2token_idx_end[end]\n",
    "                except KeyError:\n",
    "                    # Пример некорректен из-за опечатки, обнуляем наличие сущности (пропускаем)\n",
    "                    continue\n",
    "\n",
    "                valid_entities.append({\n",
    "                        **e,\n",
    "                        \"tl_start\" : new_start,\n",
    "                        \"tl_end\" : new_end\n",
    "                    })\n",
    "            new_data.append({\n",
    "                    **c,\n",
    "                    \"entities\" : valid_entities,\n",
    "                    \"tokens\" : tokens,\n",
    "                    \"type_ids\" : type_ids,\n",
    "                    \"offsets\" : offsets,\n",
    "                })\n",
    "\n",
    "        self.flat_data = new_data\n",
    "\n",
    "        # Шаг 5: Строим последовательность меток по формату IOBES\n",
    "\n",
    "        labeled_data = []\n",
    "        for c in self.flat_data:\n",
    "\n",
    "            labels = []\n",
    "\n",
    "            for e in c[\"entities\"]:\n",
    "                labels.append((e[\"tag\"], e[\"tl_start\"], e[\"tl_end\"]))\n",
    "\n",
    "            label_seq = [\"O\"] * len(c[\"tokens\"])\n",
    "\n",
    "            ### iobes\n",
    "\n",
    "            for tag, start, end in labels:\n",
    "\n",
    "                if start == end: # S\n",
    "                    label_seq[start] = 'S-' + tag\n",
    "                else: # BI*E\n",
    "                    label_seq[start] = 'B-' + tag \n",
    "                    label_seq[end] = 'E-' + tag \n",
    "                    for i in range(start + 1, end):\n",
    "                        label_seq[i] = 'I-' + tag\n",
    "\n",
    "            labeled_data.append({\n",
    "                    **c,\n",
    "                    \"labels\" : label_seq\n",
    "                })\n",
    "\n",
    "        self.data = labeled_data\n",
    "\n",
    "        # Шаг 6. Загружаем все типы сущностей и присваиваем им id\n",
    "\n",
    "        with open(ners_path, \"r\", encoding='UTF-8') as ners_file:\n",
    "\n",
    "            tags = [t.strip() for t in ners_file.read().split('\\n')]\n",
    "\n",
    "            tag_to_id = {}\n",
    "            for idx, tag in enumerate(tags):\n",
    "                tag_to_id['B-' + tag] = idx * 4 + 1\n",
    "                tag_to_id['I-' + tag] = idx * 4 + 2\n",
    "                tag_to_id['E-' + tag] = idx * 4 + 3\n",
    "                tag_to_id['S-' + tag] = idx * 4 + 4\n",
    "            tag_to_id['O'] = 0\n",
    "\n",
    "            self.tag_to_id = tag_to_id\n",
    "\n",
    "        ###\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        data = self.data[item]\n",
    "\n",
    "        assert len(data[\"tokens\"]) == len(data[\"type_ids\"])\n",
    "        assert len(data[\"type_ids\"]) == len(data[\"labels\"])\n",
    "\n",
    "        labels_ids = [self.tag_to_id[tag] for tag in data[\"labels\"]]\n",
    "\n",
    "        tokens = data[\"tokens\"]\n",
    "        type_ids = data[\"type_ids\"]\n",
    "        offsets = data[\"offsets\"]\n",
    "\n",
    "        context = data[\"context\"]\n",
    "        filename = data[\"filename\"]\n",
    "        txtdata = data[\"txtdata\"]\n",
    "        tid = data[\"tid\"]\n",
    "        c_start = data[\"c_start\"]\n",
    "        c_end = data[\"c_end\"]\n",
    "\n",
    "        # truncate\n",
    "        tokens = tokens[: self.max_length]\n",
    "        type_ids = type_ids[: self.max_length]\n",
    "        labels_ids = labels_ids[: self.max_length]\n",
    "        offsets = offsets[: self.max_length]\n",
    "\n",
    "        sep_token = self.tokenizer.token_to_id(\"[SEP]\")\n",
    "        if tokens[-1] != sep_token:\n",
    "            assert len(tokens) == self.max_length\n",
    "            tokens = tokens[: -1] + [sep_token]\n",
    "\n",
    "        return [\n",
    "            torch.LongTensor(tokens),\n",
    "            torch.LongTensor(type_ids),\n",
    "            torch.LongTensor(labels_ids),\n",
    "            torch.LongTensor(offsets),\n",
    "            torch.LongTensor([data[\"id\"]]),\n",
    "            context,\n",
    "            filename,\n",
    "            txtdata,\n",
    "            tid,\n",
    "            c_start,\n",
    "            c_end\n",
    "        ]\n",
    "\n",
    "\n",
    "def test_dataset():\n",
    "\n",
    "    dataset = IOBESFlatRuNNEDataset(dataset_name = \"dev\",\n",
    "                                    dataset_path = \"public_dat\", \n",
    "                                    ners_path = \"../eval/ref/ners.txt\",\n",
    "                                    format_path = \"public_dat/dev.jsonl\", \n",
    "                                    in_path = \"../eval\",\n",
    "                                    tokenizer = BertWordPieceTokenizer(\"vocab.txt\", lowercase = False),\n",
    "                                    max_length = 128)\n",
    "\n",
    "    # print(dataset[50])\n",
    "    print(len(dataset))\n",
    "    print(dataset[10])\n",
    "    # filenames = set([dataset[i][-5] for i in range(len(dataset))])\n",
    "    # print(len(filenames))\n",
    "\n",
    "    # dataloader = DataLoader(\n",
    "    #     dataset=dataset,\n",
    "    #     batch_size=2,\n",
    "    #     shuffle=False,\n",
    "    #     collate_fn=collate_to_max_length\n",
    "    # )\n",
    "\n",
    "    # for batch in dataloader:\n",
    "    #     print(batch)\n",
    "    #     break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
